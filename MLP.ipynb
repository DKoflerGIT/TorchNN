{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "After watching Andrej Kaparthy's videos on neural networks (https://www.youtube.com/@AndrejKarpathy), I was encouraged to play with PyTorch myself. I might have copied some code parts from him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental data: AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]]).float()\n",
    "Y = torch.tensor([[0], [0], [0], [1]]).float()\n",
    "\n",
    "print(f'{X.shape=}')\n",
    "print(f'{Y.shape=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize two neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "w1 = torch.randn((2, 8)).float()\n",
    "w1.requires_grad = True # for efficiency reasons, gradients must be activated manually for required parameters\n",
    "\n",
    "b1 = torch.randn((8,)).float()\n",
    "b1.requires_grad = True\n",
    "\n",
    "print(f'{w1=}')\n",
    "print(f'{b1=}')\n",
    "\n",
    "# layer 2\n",
    "w2 = torch.randn((8, 1)).float()\n",
    "w2.requires_grad = True\n",
    "\n",
    "b2 = torch.randn((1,)).float()\n",
    "b2.requires_grad = True\n",
    "\n",
    "print(f'{w2=}')\n",
    "print(f'{b2=}')\n",
    "\n",
    "parameters = [w1, b1, w2, b2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "\n",
    "for _ in range(n):\n",
    "    # forward pass\n",
    "    l1 = torch.tanh(X @ w1 + b1) # layer 1\n",
    "    logits = torch.tanh(l1 @ w2 + b2) # layer 2\n",
    "\n",
    "    # loss\n",
    "    loss = sum((logits - Y)**2)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters: # torch requires you to reset parameter gradients. Can be set to 0, but None is more efficient\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -0.01 * p.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tanh(torch.tanh(X @ w1 + b1) @ w2 + b2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
